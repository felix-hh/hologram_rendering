{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from lightfield_canvas import DisplayLF\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image 13 out of 104\n",
      "Loaded image 26 out of 104\n",
      "Loaded image 39 out of 104\n",
      "Ignored file: position_error_0077.tif\n",
      "Loaded image 52 out of 104\n",
      "Loaded image 65 out of 104\n",
      "Loaded image 78 out of 104\n",
      "Loaded image 91 out of 104\n",
      "Loaded image 104 out of 104\n",
      "Lightfield ready\n"
     ]
    }
   ],
   "source": [
    "# The dataset contains images captured with a camera following a zig-zag path, where the camera moves on\n",
    "# horizontal lines, and when complete, it shifts up one level and repeats in the opposite direction.\n",
    "# The name of each image starts with 4 numbers, that represent the index of the image when collected.\n",
    "# The index of the image on the horizontal line is the u coordinate. The index in the vertical line is the v coordinate\n",
    "data_folder = os.path.join(os.getenv(\"FELIX_DATA\"), \"LensCalibrated_6s\",\"LensCalibrated_6s\",\"Registered-ForFelix\")\n",
    "\n",
    "# Each folder has N_IMAGES\n",
    "N_IMAGES = len(os.listdir(data_folder))-1\n",
    "\n",
    "# Each v line has IMAGES_PER_LINE images in the u axis, and there are IMAGES_PER_VERTICAL in v. \n",
    "IMAGES_PER_LINE = 13\n",
    "IMAGES_PER_VERTICAL = N_IMAGES // IMAGES_PER_LINE\n",
    "IMAGE_DIMENSIONS = (1770, 1476)\n",
    "DTYPE = np.uint8\n",
    "DESIRED_DIMENSIONS = None\n",
    "if not DESIRED_DIMENSIONS:\n",
    "    DESIRED_DIMENSIONS = IMAGE_DIMENSIONS\n",
    "               \n",
    "# We load a lightfield with the images in the folder. \n",
    "# Initialize lightfield\n",
    "lightfield = np.ones([IMAGES_PER_LINE, IMAGES_PER_VERTICAL, DESIRED_DIMENSIONS[1], DESIRED_DIMENSIONS[0], 3], dtype= DTYPE)\n",
    "# Iterate through each image name in the folder\n",
    "completed_count = 0\n",
    "for image_name in os.listdir(data_folder):\n",
    "    # If the image name is valid\n",
    "    if image_name[:4].isnumeric():\n",
    "        # Put image in the lightfield array, resize image if necessary\n",
    "        image_idx = int(image_name[:4])\n",
    "        u_idx, v_idx = image_idx%IMAGES_PER_LINE, image_idx//IMAGES_PER_LINE\n",
    "        if v_idx %2 == 0: # maintain u_idx in different zig-zag directions left-right and right-left. \n",
    "            u_idx = IMAGES_PER_LINE - u_idx -1\n",
    "        image_data = np.array(Image.open(os.path.join(data_folder, image_name)))\n",
    "        image_data = cv2.resize(image_data, DESIRED_DIMENSIONS)\n",
    "        lightfield[u_idx, v_idx] = image_data\n",
    "        \n",
    "        #log for debugging\n",
    "        completed_count += 1\n",
    "        if completed_count % IMAGES_PER_LINE == 0:\n",
    "            print(f\"Loaded image {completed_count} out of {N_IMAGES}\")\n",
    "    \n",
    "    #If the index is not a number, continue\n",
    "    else:\n",
    "        print(\"Ignored file: \" + image_name)\n",
    "        continue\n",
    "\n",
    "lightfield = np.moveaxis(lightfield, [0,1,2,3,4],[1,0,2,3,4]) # DisplayLF is [vertical, horizontal, vertical, horizontal, rgb]\n",
    "print(\"Lightfield ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba88e8097a9d41a68e012c9559bc36fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(DisplayLF(height=441, width=531), Output(layout=Layout(border='1px solid black', width='200px')â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lightfield_display = DisplayLF(lightfield, width= 177*3, height= 147*3, sensitivity= 2)\n",
    "lightfield_display.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, use the Ollie's renderer to create new lightfields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import pytorch3d\n",
    "\n",
    "from pytorch3d.renderer.cameras import SfMPerspectiveCameras, OpenGLOrthographicCameras\n",
    "from pytorch3d.renderer import look_at_view_transform\n",
    "import LightfieldViewer as LV\n",
    "\n",
    "# Set the cuda device \n",
    "device = torch.device(\"cuda:1\")\n",
    "torch.cuda.set_device(device)\n",
    "dtype = torch.uint8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this requires pytorch3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate the cameras and capture the lightfield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIG ###\n",
    "to_render_lightfield = np.array(lightfield, dtype= np.float32)/255 #divide by 255 since original format is uint8.\n",
    "dtype = torch.float32 # must be float \n",
    "zsep = -.1; # the separation between xy and uv planes\n",
    "Np = 1770; # number of pixels in rendererd views\n",
    "Nv = 101; # the number of vertices across for the planar mesh\n",
    "\n",
    "### ZOOM ###\n",
    "cam_dist = 2\n",
    "\n",
    "### ANGLE ###\n",
    "angle_min = -20\n",
    "angle_max = 20\n",
    "angle_views = 20\n",
    "\n",
    "### ELEVATION ###\n",
    "elev_min = -20\n",
    "elev_max = 20\n",
    "elev_views = 20\n",
    "\n",
    "### CAMERA ###\n",
    "asp = 1;\n",
    "focal_length = [1, 1/asp];\n",
    "principal_point = [0, 0]\n",
    "\n",
    "views = 20;\n",
    "pp = torch.tensor(principal_point).expand(views,2)\n",
    "\n",
    "#Future: Can  use different distances for zooming. would be 6-dim array :)\n",
    "#zoom_min\n",
    "#zoom_max\n",
    "#zoom_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RTs(angles, elevs, cam_dist):\n",
    "    \"\"\"\n",
    "    Returns the rotation and translation transformations for an array of cameras \n",
    "    on a meshgrid of angles x elevs\n",
    "    \n",
    "    Input: \n",
    "    angles: numpy array of angles to sample. Nx array\n",
    "    elevs: numpy array of elevations to sample. Ny array\n",
    "    cam_dist: the distance of the camera to the object\n",
    "    \n",
    "    Output: \n",
    "    Rs: rotation transformations. (Nx, Ny, 3, 3)\n",
    "    Ts: translation transformations. (Nx, Ny, 3)\n",
    "    \n",
    "    TODO: handle non-array values for angles, elevs\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    angle_views = len(angles)\n",
    "    elev_views = len(elevs)\n",
    "    \n",
    "    dist = cam_dist*torch.ones(angle_views, dtype=dtype).view(angle_views) # distance from camera to the object\n",
    "    angles = torch.tensor(angles, dtype=dtype).view(angle_views)  # angle of azimuth rotation in degrees\n",
    "    elevs = torch.tensor(elevs, dtype=dtype).view(elev_views)\n",
    "\n",
    "    elevs, angles = torch.meshgrid(elevs, angles)\n",
    "\n",
    "    Rs = torch.empty((elev_views, angle_views, 3, 3))\n",
    "    Ts = torch.empty((elev_views, angle_views, 3))\n",
    "\n",
    "    for k in range(elev_views):\n",
    "        Rs[k], Ts[k] = look_at_view_transform(dist, elevs[k], angles[k], device=device) # (views,3,3), (views,3) \n",
    "\n",
    "    return Rs, Ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cameras(Rs, Ts, asp, focal_length, pp):\n",
    "    \"\"\"\n",
    "    Returns a list of PerspectiveCameras given Rs and Ts.  \n",
    "    \"\"\"\n",
    "    elev_views = Rs.shape[0]\n",
    "    angle_views = Rs.shape[1]\n",
    "    #generate focal length and principal point\n",
    "    fl = cam_dist*torch.tensor(focal_length).expand(angle_views,2)\n",
    "    pp = torch.tensor(principal_point).expand(angle_views,2)\n",
    "    \n",
    "    cameras = []\n",
    "\n",
    "    for k in range(Rs.shape[0]):\n",
    "        R, T = Rs[k], Ts[k]\n",
    "        C = SfMPerspectiveCameras(focal_length= fl, principal_point= pp, R= R, T= T, device= device)\n",
    "        cameras.append(C)\n",
    "    \n",
    "    return cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in tqdm.tqdm(range(3), disable=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 13, 1476, 1770, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightfield.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rendered_views(lightfield, cameras, zsep, Np, Nv, show_progress= False):\n",
    "    \"\"\"\n",
    "    Returns the rendered views for cameras at different angles and elevations. \n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for cams in tqdm.tqdm(cameras, disable= not show_progress): \n",
    "        #create lightfieldViewer\n",
    "        lighfieldViewer = LV.LightfieldViewerModel(device=device,dtype=dtype, init_cam=cams, \n",
    "                                        zsep=zsep, Np=Np, Nv=Nv, scale=2)\n",
    "\n",
    "        # the grid of lightfield coordinates\n",
    "        u = np.linspace(-1,1,8) # Nu x 1 regular grid of values\n",
    "        v = np.linspace(-1,1,13) # Nv x 1 regular grid of values\n",
    "        x = np.linspace(-1,1,1476) # Nx x 1 regular grid of values\n",
    "        y = np.linspace(-1,1,1770) # Ny x 1 regular grid of values\n",
    "\n",
    "        renderedViews = lighfieldViewer(lightfield=lightfield, u=u, v=v, x=x, y=y)\n",
    "        results.append(renderedViews)\n",
    "\n",
    "    results = np.array(results[::-1])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = np.linspace(angle_min, angle_max, angle_views) # linspace of ashow_progress= degrees\n",
    "elevs = np.linspace(elev_min, elev_max, elev_views) # linspace of elevs\n",
    "\n",
    "Rs, Ts = get_RTs(angles, elevs, cam_dist)\n",
    "cameras = get_cameras(Rs, Ts, asp, focal_length, pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [1:12:07<00:00, 216.37s/it]\n"
     ]
    }
   ],
   "source": [
    "rendered_views = get_rendered_views(to_render_lightfield, cameras, zsep, Np, Nv, show_progress= True)\n",
    "rendered_views = rendered_views[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, display the lightfield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f18b4e653e48998799370590d578a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(DisplayLF(height=441, width=531), Output(layout=Layout(border='1px solid black', width='200px')â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lightfield_display2 = DisplayLF(rendered_views, width= 177*3, height= 147*3, sensitivity= 2)\n",
    "lightfield_display2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original lightfield occupies 777 Mb\n",
      "The rendered lightfield occupies 14341 Mb\n"
     ]
    }
   ],
   "source": [
    "filesize_original_lightfield = lightfield.itemsize*lightfield.size//1024**2 # MB size\n",
    "filesize_rendered_lightfield = rendered_views.itemsize*rendered_views.size//1024**2 # MB size\n",
    "print(f\"The original lightfield occupies {filesize_original_lightfield} Mb\")\n",
    "print(f\"The rendered lightfield occupies {filesize_rendered_lightfield} Mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4d3f5ce7f4427c9fed721717c59791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(DisplayLF(width=500), Output(layout=Layout(border='1px solid black', width='200px'))))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_filename = f\"rendered_views_{elev_views}x{angle_views}x{Np}x{Np}.npy\"\n",
    "np.save(output_filename, rendered_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9a819d74f140e7af26602a324c2afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(DisplayLF(width=500), Output(layout=Layout(border='1px solid black', width='200px'), outputs=({â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test output has been saved correctly\n",
    "cocacola = np.load(output_filename)\n",
    "cocacola_display = DisplayLF(cocacola, width= 500, height= 500, sensitivity=2)\n",
    "cocacola_display.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Couldn't make LightfieldViewer generate a lightfield at a different zoom / camera distance.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lfrenderer)",
   "language": "python",
   "name": "zlfrenderer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
