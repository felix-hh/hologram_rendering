{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from lightfield_canvas import DisplayLF\n",
    "\n",
    "from lightfield_utils import load_lightfield_from_folder, resize_lightfield, crop_to_shape\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "today = datetime.date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset contains images captured with a camera following a zig-zag path, where the camera moves on\n",
    "# horizontal lines, and when complete, it shifts up one level and repeats in the opposite direction.\n",
    "\n",
    "data_folder = os.path.join(os.getenv(\"FELIX_DATA\"), \"LensCalibrated_6s\",\"LensCalibrated_6s\",\"Registered-ForFelix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lightfield of shape (11, 16, 856, 969, 3) ready\n"
     ]
    }
   ],
   "source": [
    "IMAGES_PER_LINE = 13\n",
    "raw_lightfield = load_lightfield_from_folder(data_folder, IMAGES_PER_LINE, file_suffix=\".tif\")\n",
    "\n",
    "\n",
    "## replace raw lightfield for resized lightfield\n",
    "# image_data = cv2.resize(image_data, DESIRED_DIMENSIONS)\n",
    "# look carefully at the preprocessing above. \n",
    "\n",
    "\n",
    "#load new lightfield instead. \n",
    "# raw_lightfield = np.load(\"2021_04_15_Benton_raw_lightfield.npy\", allow_pickle= True)\n",
    "\n",
    "raw_lightfield = np.load(\"2021_04_29_Benton_raw_lightfield.npy\", allow_pickle= True)\n",
    "\n",
    "IMAGE_DIMENSIONS = tuple(list(raw_lightfield.shape)[2:4])\n",
    "\n",
    "DESIRED_DIMENSIONS = (572, 440)\n",
    "if not DESIRED_DIMENSIONS:\n",
    "    DESIRED_DIMENSIONS = IMAGE_DIMENSIONS\n",
    "\n",
    "print(f\"Lightfield of shape {raw_lightfield.shape} ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols, n_rows, *_ = raw_lightfield.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightfield = resize_lightfield(raw_lightfield, DESIRED_DIMENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643acc11ff89499ca412abcb025bde46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(DisplayLF(height=441, width=531), Output(layout=Layout(border='1px solid black', width='200px')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lightfield_display = DisplayLF(lightfield, width= 177*3, height= 147*3, sensitivity= 2)\n",
    "lightfield_display.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, use the Ollie's renderer to create new lightfields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import pytorch3d\n",
    "\n",
    "from pytorch3d.renderer.cameras import SfMPerspectiveCameras, OpenGLOrthographicCameras\n",
    "from pytorch3d.renderer import look_at_view_transform\n",
    "import LightfieldViewer as LV\n",
    "\n",
    "# Set the cuda device \n",
    "device = torch.device(\"cuda:1\")\n",
    "torch.cuda.set_device(device)\n",
    "dtype = torch.uint8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this requires pytorch3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate the cameras and capture the lightfield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIG ###\n",
    "to_render_lightfield = np.array(lightfield, dtype= np.float32)/255 #divide by 255 since original format is uint8.\n",
    "dtype = torch.float32 # must be float\n",
    "zsep = -.1; # the separation between xy and uv planes\n",
    "output_dimensions = (572, 440); # number of pixels in rendererd views\n",
    "Np = max(output_dimensions)\n",
    "Nv = 101; # the number of vertices across for the planar mesh\n",
    "\n",
    "### ZOOM ###\n",
    "cam_dist = 2\n",
    "\n",
    "### ANGLE ###\n",
    "angle_min = -20\n",
    "angle_max = 20\n",
    "angle_views = 192\n",
    "\n",
    "### ELEVATION ###\n",
    "elev_min = -20\n",
    "elev_max = 20\n",
    "elev_views = 48\n",
    "\n",
    "### CAMERA ###\n",
    "asp = 1;\n",
    "focal_length = [1, 1/asp];\n",
    "principal_point = [0, 0]\n",
    "\n",
    "views = 20;\n",
    "pp = torch.tensor(principal_point).expand(views,2)\n",
    "\n",
    "#Future: Can  use different distances for zooming. would be 6-dim array :)\n",
    "#zoom_min\n",
    "#zoom_max\n",
    "#zoom_views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0) (66, 66)\n"
     ]
    }
   ],
   "source": [
    "def pad_to_shape(lightfield, new_shape):\n",
    "    _, _, current_height, current_width, _ = lightfield.shape\n",
    "    new_width, new_height = new_shape\n",
    "\n",
    "    def get_pad(old_dim, new_dim):\n",
    "        return (int(np.ceil((new_dim-old_dim)/2)), (new_dim-old_dim)//2)\n",
    "\n",
    "    width_pad = get_pad(current_width, new_width)\n",
    "    height_pad = get_pad(current_height, new_height)\n",
    "\n",
    "    print (width_pad, height_pad)\n",
    "\n",
    "    if not all([value >= 0 for pad_tuple in (width_pad, height_pad) for value in pad_tuple]):\n",
    "        raise Exception(\"The requested padded shape is smaller than the current shape, but padding can only increase dimensions\")\n",
    "\n",
    "    lightfield = np.pad(lightfield, ((0,0), (0,0), height_pad, width_pad, (0,0)))\n",
    "\n",
    "    return lightfield\n",
    "\n",
    "to_render_lightfield = pad_to_shape(to_render_lightfield, (Np, Np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_RTs(angles, elevs, cam_dist):\n",
    "    \"\"\"\n",
    "    Returns the rotation and translation transformations for an array of cameras \n",
    "    on a meshgrid of angles x elevs\n",
    "    \n",
    "    Input: \n",
    "    angles: numpy array of angles to sample. Nx array\n",
    "    elevs: numpy array of elevations to sample. Ny array\n",
    "    cam_dist: the distance of the camera to the object\n",
    "    \n",
    "    Output: \n",
    "    Rs: rotation transformations. (Nx, Ny, 3, 3)\n",
    "    Ts: translation transformations. (Nx, Ny, 3)\n",
    "    \n",
    "    TODO: handle non-array values for angles, elevs\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    angle_views = len(angles)\n",
    "    elev_views = len(elevs)\n",
    "    \n",
    "    dist = cam_dist*torch.ones(angle_views, dtype=dtype).view(angle_views) # distance from camera to the object\n",
    "    angles = torch.tensor(angles, dtype=dtype).view(angle_views)  # angle of azimuth rotation in degrees\n",
    "    elevs = torch.tensor(elevs, dtype=dtype).view(elev_views)\n",
    "\n",
    "    elevs, angles = torch.meshgrid(elevs, angles)\n",
    "\n",
    "    Rs = torch.empty((elev_views, angle_views, 3, 3))\n",
    "    Ts = torch.empty((elev_views, angle_views, 3))\n",
    "\n",
    "    for k in range(elev_views):\n",
    "        Rs[k], Ts[k] = look_at_view_transform(dist, elevs[k], angles[k], device=device) # (views,3,3), (views,3) \n",
    "\n",
    "    return Rs, Ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cameras(Rs, Ts, asp, focal_length):\n",
    "    \"\"\"\n",
    "    Returns a list of PerspectiveCameras given Rs and Ts.  \n",
    "    \"\"\"\n",
    "    elev_views = Rs.shape[0]\n",
    "    angle_views = Rs.shape[1]\n",
    "    #generate focal length and principal point\n",
    "    fl = cam_dist*torch.tensor(focal_length).expand(angle_views,2)\n",
    "    pp = torch.tensor(principal_point).expand(angle_views,2)\n",
    "    \n",
    "    cameras = []\n",
    "\n",
    "    for k in range(Rs.shape[0]):\n",
    "        R, T = Rs[k], Ts[k]\n",
    "        C = SfMPerspectiveCameras(focal_length= fl, principal_point= pp, R= R, T= T, device= device)\n",
    "        cameras.append(C)\n",
    "    \n",
    "    return cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rendered_views(lightfield, cameras, zsep, Np, Nv, show_progress= False):\n",
    "    \"\"\"\n",
    "    Returns the rendered views for cameras at different angles and elevations. \n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    u_count, v_count, x_count, y_count, c = lightfield.shape\n",
    "\n",
    "    for cams in tqdm.tqdm(cameras, disable= not show_progress): \n",
    "        #create lightfieldViewer\n",
    "        lighfieldViewer = LV.LightfieldViewerModel(device=device,dtype=dtype, init_cam=cams, \n",
    "                                        zsep=zsep, Np=Np, Nv=Nv, scale=2)\n",
    "\n",
    "        # the grid of lightfield coordinates\n",
    "        u = np.linspace(-1,1,u_count) # Nu x 1 regular grid of values\n",
    "        v = np.linspace(-1,1,v_count) # Nv x 1 regular grid of values\n",
    "        x = np.linspace(-1,1,x_count) # Nx x 1 regular grid of values\n",
    "        y = np.linspace(-1,1,y_count) # Ny x 1 regular grid of values\n",
    "\n",
    "        renderedViews = lighfieldViewer(lightfield=lightfield, u=u, v=v, x=x, y=y)\n",
    "        results.append(renderedViews)\n",
    "\n",
    "    results = np.array(results[::-1])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = np.linspace(angle_min, angle_max, angle_views) # linspace of ashow_progress= degrees\n",
    "elevs = np.linspace(elev_min, elev_max, elev_views) # linspace of elevs\n",
    "\n",
    "Rs, Ts = get_RTs(angles, elevs, cam_dist)\n",
    "cameras = get_cameras(Rs, Ts, asp, focal_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [2:47:12<00:00, 209.02s/it]  \n"
     ]
    }
   ],
   "source": [
    "rendered_views = get_rendered_views(to_render_lightfield, cameras, zsep, Np, Nv, show_progress= True)\n",
    "rendered_views = rendered_views[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, display the lightfield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_shape(lightfield, new_shape):\n",
    "    _, _, current_height, current_width, _ = lightfield.shape\n",
    "    new_width, new_height = new_shape\n",
    "\n",
    "    def get_slice(old_dim, new_dim):\n",
    "        return ((old_dim - new_dim)//2, (old_dim - new_dim)//2 + new_dim)\n",
    "\n",
    "    width_slice = get_slice(current_width, new_width)\n",
    "    height_slice = get_slice(current_height, new_height)\n",
    "\n",
    "    if not all([current_value >= new_value\n",
    "                for current_value, new_value in zip((current_height, current_width), (new_height, new_width))]):\n",
    "        raise Exception(\"The requested cropped shape is bigger than the current shape, but cropping can only decrease dimensions\")\n",
    "\n",
    "    (y0, y1), (x0, x1) = height_slice, width_slice\n",
    "\n",
    "    lightfield = lightfield[:, :, y0 : y1, x0 : x1, :]\n",
    "    return lightfield\n",
    "\n",
    "rendered_views = crop_to_shape(rendered_views, output_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1340abd3622b42b49d591760540d5a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(DisplayLF(height=441, width=531), Output(layout=Layout(border='1px solid black', width='200px')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lightfield_display2 = DisplayLF(rendered_views, width= 177*3, height= 147*3, sensitivity= 2)\n",
    "lightfield_display2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original lightfield occupies 1013 Mb\n",
      "The rendered lightfield occupies 26544 Mb\n"
     ]
    }
   ],
   "source": [
    "filesize_original_lightfield = lightfield.itemsize*lightfield.size//1024**2 # MB size\n",
    "filesize_rendered_lightfield = rendered_views.itemsize*rendered_views.size//1024**2 # MB size\n",
    "print(f\"The original lightfield occupies {filesize_original_lightfield} Mb\")\n",
    "print(f\"The rendered lightfield occupies {filesize_rendered_lightfield} Mb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 192, 440, 572, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rendered_views.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rendered_views_2021_4_29_48x192x572x440.npy\n"
     ]
    }
   ],
   "source": [
    "#note - there will be a file clash if you try to generate 2 rendered_views in the same day and don't rename.\n",
    "output_filename = f\"rendered_views_{today.year}_{today.month}_{today.day}_{elev_views}x{angle_views}x{output_dimensions[0]}x{output_dimensions[1]}.npy\"\n",
    "print(output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(output_filename, rendered_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test output has been saved correctly\n",
    "rendered_lightfield = np.load(output_filename)\n",
    "rendered_lightfield = np.nan_to_num(rendered_lightfield)\n",
    "rendered_lightfield_display = DisplayLF(rendered_lightfield, width= 572, height= 440, sensitivity=2)\n",
    "rendered_lightfield_display.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "output_folder = Path(\".\") / \"rendered_views\" / output_filename[:-4]\n",
    "\n",
    "def save_rendered_views(image_folder):\n",
    "    \n",
    "    image_folder.mkdir(parents= True, exist_ok=True)\n",
    "\n",
    "    for row_idx, row in enumerate(tqdm.tqdm(rendered_lightfield)):\n",
    "        for column_idx, image_data in enumerate(row):\n",
    "            image = Image.fromarray(np.uint8(image_data*256))\n",
    "            image.save(image_folder / f\"{row_idx:04d}_{column_idx:04d}.png\", format= \"png\", opt= \"yes\", quality= 75)\n",
    "\n",
    "    print(\"Saved at\", str(image_folder.absolute()))\n",
    "\n",
    "save_rendered_views(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boxsdk import DevelopmentClient, OAuth2, Client\n",
    "from boxsdk.network.default_network import DefaultNetwork\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Define client ID, client secret, and developer token.\n",
    "CLIENT_ID = None\n",
    "CLIENT_SECRET = None\n",
    "ACCESS_TOKEN = None\n",
    "\n",
    "# Read app info from text file\n",
    "with open('box_credentials.txt', 'r') as app_cfg:\n",
    "    \"\"\"\n",
    "    This is a .txt file with 3 lines\n",
    "    CLIENT_ID\n",
    "    CLIENT_SECRET\n",
    "    ACCESS_TOKEN (the developer token)\n",
    "    find at https://northwestern.app.box.com/developers/console/app/1484448/configuration \n",
    "    \"\"\"\n",
    "    CLIENT_ID = app_cfg.readline()\n",
    "    CLIENT_SECRET = app_cfg.readline()\n",
    "    ACCESS_TOKEN = app_cfg.readline()\n",
    "\n",
    "oauth2 = OAuth2(CLIENT_ID, CLIENT_SECRET, access_token=ACCESS_TOKEN)\n",
    "\n",
    "# Create the authenticated client\n",
    "client = Client(oauth2)\n",
    "root_folder = client.folder('0').get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_folder = None\n",
    "\n",
    "for item in root_folder.get_items():\n",
    "    print(item)\n",
    "    if item.name==\"LensCalibrated_6s\":\n",
    "        target_folder = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new folder for the rendered views\n",
    "tmp_filename = \"tmp_\" + str(time.time()) + \".zip\"\n",
    "shutil.make_archive(tmp_filename[:-4], 'zip', output_folder)\n",
    "#upload zip to box\n",
    "target_folder.upload(tmp_filename, output_folder.name+\".zip\")\n",
    "\n",
    "os.remove(tmp_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lfrenderer)",
   "language": "python",
   "name": "zlfrenderer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
